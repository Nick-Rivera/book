\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ...
\usepackage{graphicx}
\usepackage{framed}
\usepackage{enumitem}
\usepackage{url}
\usepackage{natbib}
\newlist{legal}{enumerate}{10}
\setlist[legal]{label*=\arabic*.}

\usepackage[hidelinks]{hyperref}

% Font
\usepackage[sc, osf]{mathpazo}
\linespread{1.05} % to accomodate palatino
\usepackage[T1]{fontenc} % accented chars, etc...

\setcounter{footnote}{1}

\title{Declaring and Diagnosing Research Designs \\ {\normalsize A Book Proposal to Princeton University Press}}
\author{Graeme Blair\footnote{Assistant Professor of Political Science, UCLA. \href{mailto:graeme.blair@ucla.edu}{graeme.blair@ucla.edu}. \url{https://graemeblair.com}} \quad 
Jasper Cooper\footnote{Ph.D. candidate in Political Science, Columbia University. \href{mailto:jjc2247@columbia.edu}{jjc2247@columbia.edu}. \url{http://jasper-cooper.com}} \quad 
Alexander Coppock\footnote{Assistant Professor of Political Science, Yale University. \href{mailto:alex.coppock@yale.edu}{alex.coppock@yale.edu}. \url{https://alexandercoppock.com}} \quad 
\setcounter{footnote}{7}
Macartan Humphreys\footnote{Professor of Political Science, Columbia University. \href{mailto:mh2245@columbia.edu}{mh2245@columbia.edu}. \url{http://www.macartan.nyc}}   }

\begin{document}
\maketitle


\section{Proposal}

\noindent \textit{Declaring and Diagnosing Research Designs} will:
\begin{enumerate}
	\item Introduce a framework for conceptualizing designs in a demonstrably \textit{complete} way
	\item Guide readers in the formal declaration and interrogation of research designs
	\item Provide a library of standard and complex research designs commonly employed in the social sciences, which readers can adopt or modify for their own research
	\item Use declared research designs to highlight design tradeoffs and illustrate major principles of research design
\end{enumerate}

The book builds on two ideas. First, designs can be ``declared'' by formally stating design features in the proposed <\textit{Model}, \textit{Inquiry}, \textit{Data Strategy}, \textit{Answer Strategy}> framework (MIDA). Second, once declared, designs can be ``diagnosed.'' Design diagnosis makes it possible to investigate the quality of the answers provided by a design under a variety of conditions.   

The core idea of formally declaring a design is that a design can be treated as self-contained object that can be interrogated. Once thought of as an object in this way a design can be shared, modified, and used. Our goal is to alter the way readers think about research designs, whether or not they end up using our accompanying tools. We hope that the framework will support research transparency, facilitate design development and evaluation, and help structure critique. 

\section{Format}

We propose that the book be published as a hardcopy text-treatise and accompanied by a comprehensive set of open-access online materials. We hope the online materials will make the book more valuable to readers, and vice-versa. The online materials would include:

\begin{enumerate}
	\item A complete online version of the book, organized as a website, a development version of which can be found at \url{http://DDRD.DeclareDesign.org}.
	\item Interactive versions of most visualizations included in the physical book. An example would be a power curve that shifts depending on other design parameters.
	\item A design library that will be continuously updated and expanded. We will encourage readers of the book to contribute their own designs back to the design library.
\end{enumerate}

In addition to the online materials, the book will be accompanied by a set of three software tools:

\begin{enumerate}
\item The \texttt{DeclareDesign} package, an \texttt{R}  package which is used throughout the book to declare and diagnose designs
\item The ``Design Declarer'' web interface that allows readers to generate and diagnose designs without using  \texttt{R}  
\item The ``Design Inspector'' web interface that allows readers to read, modify, and use designs without using  \texttt{R}, including those in the design library
\end{enumerate}

The online materials will integrate these applications naturally, coupling text and figures with interactive computational tools that let readers explore the properties of research designs in real time. 

\section{Motivation}

Advances in causal analysis from the graphical models and potential outcomes frameworks have revolutionized empirical social science research. In recent years, focus has shifted to more careful definition of causal quantities of interest and a more direct focus on study design as a basis for inference.

Experimental research in particular has experienced enormous growth in the past two decades. For many years, whole disciplines were viewed as non-experimental sciences; this is no longer the case. Likewise, observational quantitative research has shifted from wholesale modeling of outcomes on the basis of large sets of predictors to explicitly seeking to approximate randomized experiments through the use of matching, regression discontinuity, and instrumental variable designs. The insights from the ``identification revolution'' have also illuminated descriptive research practices --- not least by clearly delineating how causal questions differ from descriptive questions. More strikingly, there has been a growth in the use of experimental techniques as measurement devices. This influence can also be seen in some qualitative research practices also, with a move towards defining research goals in terms of causal estimands, articulating mappings from data to inferences, and in some cases pre-registering qualitative strategies. 

Researchers, whether focused on observational or experimental approaches, are increasingly paying close attention to the properties of research designs and the ability of these designs to justify inferences. In doing so, they face two challenges.  

First, surprisingly little attention has been paid to the fundamental question of what constitutes a design. This lack of clarity carries risks both before and after the implementation of a study. If designs are incompletely specified ex ante, it is difficult for researchers to assess their strengths and improve them. If designs are incompletely specified at the time of analysis, researchers may choose inappropriate analysis procedures or worse, may only report the most attractive model from a set of possible specifications. If designs remain unspecified after analysis, it may be difficult for other scholars to replicate a study or to judge whether a given type of reanalysis is justified. 

A second problem compounding the first is that few tools exist for assessing properties of designs comprehensively. At one extreme, a minority of advanced scholars conduct fully-fledged simulations requiring programming skills beyond the capabilities of many applied researchers. At the other, researchers resort to basic power calculators that cannot capture important features of the research setting, such as the sampling or assignment strategy. We know of no readily-available methods to investigate whether a design will be biased, have good coverage, or meet other important inferential criteria beyond power. 

{\it DDRD} addresses these two problems. It introduces a framework to describe the distinct elements of a research design. Using the framework, it provides a library of common designs, fully characterized. It goes on to use the design library to elucidate 50 general principles of research design.
\newpage
\section{The Book}

The book is organized in four main sections as described in Box 1 below. We describe the goals of each section in turn, and present a detailed (draft) plan in Appendix \ref{app:toc}.

\begin{framed}
\begin{centering} Box 1 \end{centering}\\

	\noindent\textbf{Contents}
	\begin{enumerate}
		\item[A]: Declaring and Diagnosing Research Designs
		
		\begin{enumerate}
			\item[1] MIDA, in principle, in practice [20 pages]
			\item[2] Diagnosing research designs [8 pages]
			\item[3] How to read this book [4 pages]
		\end{enumerate}
		
		\item[B]: Design Library
		\begin{enumerate}
			\item[4] Observational designs for descriptive inference [30]
			\item[5] Experimental designs for descriptive inference [30]
			\item[6] Experimental designs for causal inference [30]
			\item[7] Observational designs for causal inference [30]
		\end{enumerate}
		
		\item[C]: Principles of Research Designs
		\begin{enumerate}
			\item[8] Model principles [30]
			\item[9] Inquiry principles [50]
			\item[10] Data strategy principles [60]
			\item[11] Analysis strategy principles [60]
			\item[12] Cross-MIDA principles [40]
		\end{enumerate}
	\item[D]: Putting Declared Designs to Use
				\begin{enumerate}
					\item[13] Researchers: Design development and registration [8 pages]				
					\item[14]  Peers: Better scholarly critique [8 pages]
          \item[15]  Funders: Evaluating and supporting research [6 pages]   
				\end{enumerate}
				
	\end{enumerate}
	[420 pages total, approx.]
\end{framed}


\subsection{The MIDA Framework}

Part A introduces the MIDA framework for characterizing research designs, which builds on ideas about research designs in~\cite{king1994designing}. The framework conceptualizes a design as comprising four elements $<M,I,D,A>$:
\begin{enumerate}
	\item A \textbf{model}, $M$, of how the world works. The model specifies the moving parts --- the variables --- and how these are causally related to each other. In this sense the model provides the context of a study, but also a speculation about the world.    
	\item An \textbf{inquiry}, $I$, about the distribution of variables, perhaps given interventions on some variables.  In many applications $I$ might be thought of as the ``estimand.'' Some inquiries are statements about the values of variables, others about the causal relations between variables. In all cases however the inquiry should be answerable given the model.   
	\item A \textbf{data} strategy, $D$, generates data on variables.  Note that implicitly the data strategy includes case selection, or sampling decisions, but it also represents interventions such as assignment of treatments or measurement strategies. A model $M$ tells you what sort of data you might observe if you employ data strategy $D$.  
	\item An \textbf{answer} strategy, $A$, that uses data to generate an answer.  
\end{enumerate}

MIDA is deliberately pitched at a high level of generality so that it can be relevant for research whether it seeks to answer causal or descriptive questions, uses experimental or observational data, or employs quantitative or qualitative inference strategies.

% This should be $M$ and $D$ only?
A key feature of this bare specification is that if $M$, $D$, and $A$ are sufficiently well described, the answer the design provides to question $I$ has a well defined distribution. Moreover, one can construct a distribution of comparisons of the answer provided by the answer strategy ($a^A$) to the correct answer under $M$ ($a^M$). 

The ability to calculate distributions of answers, given a model, opens multiple avenues for assessment and critique of research strategies. How good is the answer you expect to get from this strategy? Would you do better with a different data strategy? With a different analysis strategy? How good is the strategy if the model is wrong in some way or another? 

We call this \textit{design diagnosis}. The key idea of design diagnosis is that if one has a fully specified design one can predict what will happen when a design is implemented, at least conditional on the assumptions of the design. A design is ``fully-specified'' if it can be simulated. From there it is an easy step to assess what would result from multiple implementations of a design and so build up a distribution of quantities of interest --- what we term ``diagnosands.'' Researchers are familiar with the idea of statistical power --- the probability that a design will produce a $p$-value below some specified cutoff. That same idea can be extended to many properties of a design: once specified, repeated iteration of the design reveals its bias, the precision of the estimates, the performance of one design relative to another in a given setting, and so on. 

Alongside the introduction of the core ideas of design declaration and diagnosis, Part A provides a general introduction to the \texttt{DeclareDesign} package and web applications, describing how readers can characterize designs using inbuilt functionality or external tools.


\subsection{Designs, Diagnosed}

Part B introduces a design library. The characterization of designs we advocate allows for arbitrarily complex and idiosyncratic designs. Many studies however use quite standard design types. In this section we provide a characterization of 25 common designs, describing the assumptions behind them and their basic properties. Defined as objects early in the book, these same designs can be reused or modified to explore design principles later on. 

Each design is: (a) introduced in terms of the MIDA framework; (b) formally characterized in the text; (c) illustrated; and (d) accompanied by short set of exercises inviting the reader to consider non-obvious features of the designs. In addition, the text points to examples of where these designs have been used as well as possible applications of the designs.

\subsection{Principles}

Part C introduces general principles of research design. We use the framework from Part A and the designs declared in Part B to introduce a set of approximately 50 principles of research that cover data gathering, question formation, and data analysis. We seek principles that have general import --- that is, that may be relevant for any research strategy --- but that are under-appreciated.

We believe the strength of the book is to be able to use the framework developed in earlier sections to show how these principles matter for specific research designs and how modifiable details of the designs can make them more or less salient. 

In the accompanying sample we provide examples of these principles and in the detailed table of contents we provide a draft listing of 50 possible principles. These are all principles that have (a) general import even if they are not broadly adhered to and (b) are well illustrated by design declaration --- that is, declaration and diagnosis of designs shed light on when concerns raised by these principles are more or less important.

Each of these principles will be accompanied by (a) a design; (b) a figure illustrating the principle; and (c) short exercises that challenge readers to explore the principles further or in other domains. 

\subsection{Putting Declared Designs to Use}
The final part of the book is the shortest part and points readers to broader uses of the book and framework, beyond the central role for researchers seeking to develop strong designs. The section has three chapters focused on three different audiences.

The first chapter, aimed at researchers, discusses the linkages between design declaration and pre-registration. The advance characterization of a design can greatly facilitate the registration process and in this section we describe how this is best done. 

The second chapter focuses on the role that design declaration can have for scholarly critique. A common approach to scholarly critique is to alter some feature of an analysis strategy and to assess how results change. The problem with this approach is that when divergent results are found, third parties do not have clear grounds to decide which results to believe. A more coherent strategy facilitated by design declaration is to learn about what a study {\it could have} revealed under different analysis strategies, not just what the original author, or a critic, reports {\it was} revealed. Answering this question provides grounds for supporting alternative analyses on the basis of the original authors' intentions and not on the basis of the degree of divergence of results. Conversely, it provides authors with grounds to question claims made by their critics. 

The third chapter focuses on research funders. We believe that design declaration can alter the way that research designs are evaluated. Currently funders of research have limited tools for assessing  research quality, relying largely on the reputations of researchers. Sometimes power analyses are requested though these are often difficult to appraise. In contrast, access to declared designs allows funders to directly interrogate a design, to check whether the design will provide reliable learning under different conditions or if operated at different scales, under assumptions provided by researchers. This chapter describes how such review and evaluation activities can be conducted with declared designs.

\section{Relation to Existing Work}

There are many excellent textbooks that deal with different features of design. \textit{DDRD} differs from these broadly in terms of (a) the integrated approach taken to conceptualizing design, rather than (only) a collection of tools; and (b) its hands-on approach to developing and diagnosing designs.

\textbf{Integrated framework.} Many design books focus on particular dimensions of research designs; many on analysis strategies, some on data collection strategies, such as sampling or assignment strategies. \textit{DDRD} differs from these in using an integrated approach where data strategies are assessed in light of analysis strategies and vice versa. In this sense it is closer to books on design such as \citet{gerring2011social} and \citet{brady2010rethinking}, although it takes a more formal approach. We note that some books take an even more expansive view of research design. \citet{glennerster2013running} for example includes sections on logistical details of field research such as engaging with partners, for example. Our book focuses more narrowly on the analytic properties of designs, though doing so in a more formal and technically-integrated way. 

A difference with many design books is that \textit{DDRD} is not comprehensive of a particular set of strategies, and it is not intended to be. Although many important workhorse designs used in social sciences are covered, the goal is not to cover all possible designs. The set of principles illustrated are somewhat eclectic. The reason for this is that the book seeks less to deliver a set of results and more to provide researchers with a strategy and a tool to address unanticipated design questions.  

\textbf{Hands on approach.} \textit{DDRD} is also hands-on in the sense of developing principles in an applied context. For this, it shares with many other books a strategy of integrating learning about general principles with the use of statistical software. These include \citet{gelman2006data} and  \citet{james2014introduction}, though both of these focus primarily on analysis methods.  
\textit{DDRD} instead focuses on designs in an integrated way --- not as a checklist, but as a collection of relations between objects that need to be mutually coherent. In this sense, it is close to books such as \citet{Gerber2012} and \citet{dunning2012natural} that have a goal of helping researchers use causal models to think about the inferences that can be made from different sorts of data generating strategies and insisting on the importance of maintaining consistency between data generation strategies and analysis strategies. It differs from these in its more practical focus, aimed at helping researchers develop complete designs and validating them, as well as linking general principles to specifics of designs. 

Like the classic text \citet{king1994designing}, \textit{DDRD} takes a holistic view of research design: our framework can be applied to empirical social scientific work in many traditions, including most qualitative and quantitative approaches. We build on \citet{king1994designing} by applying insights from the causal inference ``revolution'' across a wide variety of research settings.

In all we believe that \textit{DDRD} occupies an unique place among current books on design, offering a generally applicable framework for thinking about designs in an integrated way and a set of practical tools to develop designs and learn about their properties. We know of no other book that seeks to do this.

\clearpage
\section{Audience}

\textit{DDRD} is not intended as a standalone textbook for a course. Instead, it serves as an accompanying text, which could be used in a wide variety of courses, and as a reference for researchers and funders. Likely courses at institutions that \textit{DDRD} could accompany include:

\begin{enumerate}
\item General research design courses in masters and Ph.D. programs in political science, public policy, sociology, and business schools
\item Experiments and causal inference courses in Ph.D. programs in political science, public policy, and sociology
\item Advanced experiments and quantitative methods courses at top undergraduate institutions
\item Applied substantive courses in the social sciences that feature complex applied research designs
\end{enumerate}

For these classes the book could serve as a set of modules, focusing students on fundamentals of research design, or it could be used to accompany other statistical classes, providing tools for students to explore the properties of any model that is introduced. Beyond methods courses, many graduate courses in the social sciences focus on published research with highly complex protocols and methods. The findings of such research are difficult to interpret without some understanding of the underlying design. Our short-format overviews could be given as handouts to provide an introduction to the underlying logic of the methods employed, as well as their advantages and disadvantages. 

Beyond the classroom the book is aimed at evaluation practitioners --- impact evaluation and measurement and evaluation (M\&E) specialists in government, international lending institutions such as the World Bank, bilateral aid agencies, major donors, and political consulting organizations such as the Analyst Institute ---  who all support research but have few tools to assess the integrity of research proposed to them.


\section{Timeline}

We expect to complete the book over the next 18 months, with research and programming assistance funded in part by the Laura and John Arnold Foundation.

We have some milestones set for the manuscript development:

\begin{itemize}
\item 	Graeme Blair will teach a course around this content at UCLA in Winter 2018 and 2019 using 15 declared designs and 15 elaborated principles
\item 	Alex Coppock will teach a course around this content at Yale in Spring 2018 and 2019 using 15 declared designs and 15 elaborated principles
\item  By October 2018, we will complete a draft of the book material and hold a book conference (funded by the Arnold Foundation) shortly thereafter
\item We will then complete the remainder of the manuscript by April 2019
\end{itemize}

\section{Enclosures}

We enclose:

\begin{enumerate}
\item Sample pages containing two subsections of Part B (``Simple Random Sampling,'' ``Regression Discontinuity Designs'') and one subsection of Part C (``Questions Should Have Well-Defined Answers'').  The principle we provide as an example for Part C is one that emerges naturally from the integrated approach to design development we describe; in standard approaches when estimators are defined separately from estimates it can be easy to implement analysis without noticing that the estimand itself is not well defined.  
\item A prototype of the e-book, including interactive applications that interrogate various research designs. This prototype can be viewed at \url{http://DDRD.DeclareDesign.org}; an example of the inspector application integrated into the book can be seen at \url{http://DDRD.DeclareDesign.org/simple-random-sampling.html}.
\item A draft detailed table of contents (Appendix to this document)
\item An unpublished article manuscript outlining the framework to be expanded upon in the book, available at \url{http://declaredesign.org/paper.pdf}
\end{enumerate}	

\clearpage\newpage


\appendix
\section{Appendix: Detailed (Draft) Table of Contents \label{app:toc}}

\begin{legal}
\item {Part A: Declaring and Diagnosing Research Designs}

	\begin{legal}
	\item  MIDA, in principle, in practice
	\item  Diagnosing research designs
	\item  How to read this book
	\end{legal}

\item Part B: Design Library

	\begin{legal}
	\item Observational designs for descriptive inference
	
		\begin{legal}
		\item Simple random sampling
		\item Stratified clustered random sampling
		\item Measuring latent variables
		\item Topic analysis
		\item Respondent driven sampling
		\item Multilevel regression and poststratification
		\end{legal}
	
	\item Experimental designs for descriptive inference
	
		\begin{legal}
		\item 	Audit experiments
		\item 	List experiments
		\item 	Randomized response
		\item 	Conjoint experiments
		\item 	Experimental games
		\end{legal}
	
	\item Experimental designs for causal inference
	
		\begin{legal}
		\item 	Multiarm designs
		\item 	Experiments with blocks and clusters
		\item 	Factorial designs
		\item 	Encouragement designs 
		\item 	Stepped wedge designs		
   	    \item 	Crossover designs
   	    \item   Parallel design for mediation effects
		\item 	Partial population design for spillover analysis
		\item 	Selective trials
		\item         Adaptive trials 
		\end{legal}
\newpage
\vspace{-5mm}
	\item Observational designs for causal inference
	
		\begin{legal}
		\item   Difference in differences
		\item 	Matching
		\item 	Regression discontinuity designs
		\item 	Synthetic control
		\item 	Process tracing
		\item 	Cross national time series
		\end{legal}
	\end{legal}

\item Part C: Principles of Research Designs
	
	\vspace{-2mm} % Making things fit on one page
	
	\begin{legal}
	\item Model principles
	
		\begin{legal}
		\item 	Diagnose your design under a null model 
		\item 	Make your models vulnerable 
		\item 	Specify which variables are manipulable
		\item 	Spillovers are potential outcomes
		\item 	Compliance is a potential outcome
		\item 	Attrition is a potential outcome
		\end{legal}
		
	\item Inquiry principles:
		
		\begin{legal}
					
		\item	There is no causation without manipulation
		\item	Answers to inquiries should be defined 
		\item	Know what ATE averages over
		\item	Know what is local in a LATE 
		\item	Know when SATEs are informative for PATEs
		\item	Discovery procedures can be declared as inquiries
		\item	Decisions can be defined as inquiries 
		\end{legal}
				
		\item Data strategy principles 

		\begin{legal}				
		\item	Systematic data collection is not random sampling
		\item	Keep treatment and control groups parallel
		\item	Blocking improves precision
		\item	Clusters reduce  precision
		\item	Estimation strategies matter for precision too
		\item	Pretreatment information can improve precision
		\item	Allocating treatments at extremes can improve precision
		\item	Multiple measurements can improve precision
		\item	Factorial designs can be more powerful than multiarm designs
		\end{legal}
		
    \item Analysis strategy principles 
		
    \begin{legal}
		\item 	Balance tests are different from randomization checks 
		\item   Assess imbalance on substantive rather than statistical grounds
		\item 	Unbiasedness is not affected by imbalance but conditional bias is
		\item 	Use controls for prognostic pretreatment covariates
		\item 	Pretreatment controls can introduce bias in observational analysis
		\item 	Conditioning on post treatment variables can introduce bias
		\item 	Heterogeneous treatment propensities cannot be ignored
		\item 	The source of uncertainty matters for inference
		\item 	The test statistic does not imply a null
		\item 	Neyman variance is conservative for sample variance
		\item 	You should usually cluster standard errors at the level of treatment assignment
		\item	Test selection should take account of both validity and power 
		\item	Unbiased variance estimates do not mean correct $p$ values
		\item	You can check whether confidence intervals are correct, given the model 
		\item	No evidence of an effect is not evidence of no effect
		\item	The difference between significant and not significant is not significant
		\end{legal}
		
		\item Cross-MIDA principles
		
		\begin{legal}
		\item	As ye randomize so shall ye analyze
		\item	Don't confuse observational and experimental variation
		\item	Selective reporting introduces bias
		\item	There is often a bias-variance tradeoff
		\item	Treat spillovers as a design challenge
		\item	Asking many questions is costly			
		\item	Models can be validated, not just assumed
		\item Diagnoses can reveal whether implementing your study is worthwhile
		\end{legal}
	\end{legal}
	
\item Part D: Putting Declared Designs to Use

	\begin{legal}
	\item  Researchers: Design development and registration
	\item  Peers: Better scholarly critique
	\item  Funders: Evaluating and supporting research 
	\end{legal}


\item Appendices
\end{legal}

\newpage

\bibliography{bib}
\bibliographystyle{apsr}
\end{document}
