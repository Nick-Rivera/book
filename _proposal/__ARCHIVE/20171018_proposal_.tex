\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ...
\usepackage{graphicx}
\usepackage{framed}
\usepackage{enumitem}
\usepackage{url}

\newlist{legal}{enumerate}{10}
\setlist[legal]{label*=\arabic*.}


\title{Declaring and Diagnosing Research Designs \\ {\normalsize A Book Proposal to Princeton University Press}}
\author{Graeme Blair \quad Jasper Cooper \quad Alexander Coppock \quad Macartan Humphreys}

\begin{document}
\maketitle


\section{Proposal}

\noindent \textit{Declaring and Diagnosing Research Designs} will:

\begin{enumerate}
	\item Introduce a framework for conceptualizing designs in a demonstrably \textit{complete} way
	\item Guide users in the formal declaration and investigation of research designs in code
	\item Provide a library of standard and complex research designs commonly employed in the social sciences, which users can adopt or modify for their own research
	\item Use declared research designs to highlight subtle design tradeoffs and illustrate major principles of research design
\end{enumerate}

The book builds on two simple ideas. First, designs can be ``declared'' in a complete manner, and treated as self-contained objects. Second, once declared, designs can be diagnosed: that is, it becomes possible to investigate the quality of the answers they provide to a research question under a variety of conditions.   

The book is designed as an open source e-book---with accompanying hard copy issue---that will serve as a go-to reference for researchers seeking to develop designs, consumers seeking to understand subtle features of existing designs, and funders trying to assess whether proposed research studies are capable of delivering on the claims made of them. Companion to the book is a software suite---\texttt{DeclareDesign}---and two general web interfaces---\texttt{DeclareDesignWizard} and \texttt{DeclareDesignInspector}---that enable users to characterize and interrogate research designs. The e-book will couple text and figures with a set of interactive computational tools that let readers explore the properties of research designs in real time. 

\section{Motivation}

Advances in causal analysis from the graphical models and potential outcomes frameworks have revolutionized empirical social science research. In recent years focus has shifted to more careful definition of causal quantities of interest and a more direct focus on study design as a basis for inference.

Experimental research in particular has experienced enormous growth in the past two decades. For many years, whole disciplines were viewed as non-experimental sciences; this is no longer the case. Likewise, observational quantitative research has shifted from wholesale modeling of outcomes on the basis of large sets of predictors to explicitly seeking to approximate randomized experiments through the use of matching, regression discontinuity, and instrumental variable designs. The insights from the ``identification revolution'' have also illuminated descriptive research practices -- not least by clearly delineating how causal questions differ from descriptive questions. More strikingly, there has been a growth in the use of experimental techniques as measurement devices. This influence can also be seen in some qualitative research practices also, with a move towards defining research goals in terms of causal estimands, articulating mappings from data to inferences, and in some cases pre-registering qualitative strategies. 

Thus authors---and readers---of empirical research are increasingly paying close attention to the properties of research designs and the ability of these designs to justify inferences. In doing so, they face two challenges.  

First, surprisingly little attention has been paid to the fundamental question of what constitutes a design. This lack of clarity carries risks both before and after the implementation of a study. If designs are incompletely specified ex ante, it is difficult for researchers to assess their strengths and improve them. If designs are incompletely specified at the time of analysis, researchers may choose inappropriate analysis procedures or worse, may only report the most attractive model from a set of possible specifications. If designs remain unspecified after analysis, it may be difficult for other scholars to replicate a study or to judge whether a given type of reanalysis is justified. 

A second problem compounding the first is that few tools exist for assessing properties of designs comprehensively. At one extreme, a minority of advanced scholars conduct fully-fledged simulations requiring programming skills beyond the capabilities of many applied researchers. At the other, researchers resort to basic power calculators that cannot capture important features of the research setting, such as the sampling or assignment strategy. We know of no readily-available methods to investigate whether a design will be biased, have good coverage, or meet other important inferential criteria beyond power. 

DDRD addresses these two problems. It introduces a framework, building on a structure outline in King, Keohane, and Verba (1994), to describe the distinct elements of a research design. Using the framework, it provides a library of common designs, fully characterized. It goes on to use the design library to elucidate 50 general principles of research design.
\newpage
\section{The Book}

The book is organized in four main sections as described in Box 1 below. We describe the goals of each section in turn, and present a detailed plan in Appendix \ref{app:toc}.

\begin{framed}
\begin{centering} Box 1 \end{centering}\\

	\noindent\textbf{Contents}
	\begin{enumerate}
		\item[A]: Declaring and Diagnosing Research Designs
		
		\begin{enumerate}
			\item[1] MIDA, in principle, in practice [10 pages]
			\item[2] Diagnosing research designs [4 pages]
			\item[3] How to read this book [4 pages]
		\end{enumerate}
		
		\item[B]: Design Library
		\begin{enumerate}
			\item[4] Observational designs for descriptive inference [20]
			\item[5] Experimental designs for descriptive inference [20]
			\item[6] Experimental designs for causal inference	 [20]
			\item[7] Observational designs for causal inference [20]
		\end{enumerate}
		
		\item[C]: Principles of Research Designs
		\begin{enumerate}
			\item[8] Model principles [20]
			\item[9] Inquiry principles [30]
			\item[10] Data strategy principles [40]
			\item[11] Analysis strategy principles [40]
			\item[12] Cross-MIDA principles [20]
		\end{enumerate}
	\item[D]: Putting Declared Designs to Use
				\begin{enumerate}
					\item[13] Researchers: Design development and registration [8 pages]				
					\item[14]  Peers: Better scholarly critique [8 pages]
          \item[15]  Funders: Evaluating and supporting research [6 pages, the Inspector App]   
				\end{enumerate}
				
	\end{enumerate}
	[300 pages total, approx]
\end{framed}


\subsection{The MIDA Framework}

Part A introduces the MIDA framework for characterizing research designs. The framework conceptualizes a design as comprising four elements $<M,I,D,A>$:
\begin{enumerate}
	
	\item A \textbf{model}, $M$, of how the world works. The model specifies the moving parts -- the variables -- and how these are causally related to each other. In this sense the model provides the context of a study but also a speculation about the world.    
	\item An \textbf{inquiry}, $I$, about the distribution of variables, perhaps given interventions on some variables.  In many applications $I$ might be thought of as the ``estimand.'' Some inquiries are statements about the values of variables, others about the causal relations between variables. In all cases however the inquiry should be answerable given the model.   
	\item A \textbf{data} strategy, $D$, generates data on variables.  Note that implicitly the data strategy includes case selection, or sampling decisions, but it also represents interventions such as assignment of treatments or measurement strategies. A model $M$ tells you what sort of data you might observe if you employ data strategy $D$.  
	\item An \textbf{answer} strategy, $A$, that uses data to generate an answer.  
\end{enumerate}

% This should be $M$ and $D$ only?
A key feature of this bare specification is that if $M$, $D$, and $A$ are sufficiently well described, the answer the design provides to question $I$ has a well defined distribution. Moreover  one can construct a distribution of comparisons of the answer provided by the answer strategy ($a^A$) to the correct answer under $M$ ($a^M$).

The ability to calculate distributions of answers, given a model, opens multiple avenues for assessment and critique of research strategies. How good is the answer you expect to get from this strategy? Would you do better with a different data strategy? With a different analysis strategy? How good is the strategy if the model is wrong in some way or another? 

We call this \textit{design diagnosis}. The key idea of design diagnosis is that if one has a fully specified design---a design that is `complete' in that it can be simulated---one can predict what will happen when a design is implemented, at least conditional on the assumptions of the design. From there it is an easy step to assess what would result from multiple implementations of a design and so build up a distribution of quantities of interest---what we term ``diagnosands.'' Researchers are familiar with the idea of statistical power---the probability that a design will produce a $p$-value below some specified cutoff. That same idea can be extended to many properties of a design: once specified, repeated iteration of the design reveals its bias, the precision of the estimates, the relative performance of one design relative to another in a given setting, and so on. 

Alongside the introduction of the core ideas of design declaration and diagnosis, Part A provides a general introduction to the \texttt{DeclareDesign} package and wizards, describing how users can characterize design using inbuilt functionality or external tools.


\subsection{Designs, diagnosed}

Part B introduces a design library. The characterization of designs we advocate allows for arbitrary complex and idiosyncratic designs. Many studies however use quite standard design types. In this section we provide a characterization of 30 common designs, describing the assumptions behind them and their basic properties. Defined as objects early in the book these same designs can be reused or modified to explore design principles later on. 

Each design is: (a) introduced in terms of the MIDA framework; (b) formally characterized in the text; (c) illustrated; and, (d) accompanied by short set of exercises inviting the reader to consider non obvious features of the designs. In addition, the text points to examples of where these designs have been used as well as possible applications of the design.

\subsection{Principles}
Part C is the heart of the book. We use the framework from Part A and the designs declared in Part B to introduce a set of approximately 50 principles of research that cover data gathering, question formation, and data analysis. We seek principles that have general import --- that is that may be relevant for any research strategy --- but that are underappreciated.

We believe the strength of the book is to be able to use the framework developed in earlier sections to show how these principles matter for specific research designs and how modifiable details of the designs can make them more or less salient. 

In the accompanying sample we provide examples of these principles and in the detailed table of contents we provide a draft listing  of 50 possible principles. Each of these principles will be accompanied by a design, a figure illustrating the principle, and short exercises that challenge readers to explore the principles further or in other domains. 

\subsection{Putting Declared Designs to Use}
The final part of the book is the shortest part and points users to broader uses of the book and framework, beyond the central role for researchers seeking to develop strong designs.

The section has three chapters focused on three different audiences.

The first chapter, aimed at researchers, discusses the linkages between design declaration and pre-registration. The advance characterization of a design can greatly facilitate the registration process and in this section we describe how this is best done. 

The second chapter focuses on the role that design declaration can have for scholarly critique. A common approach to scholarly critique is to alter some feature of an analysis strategy and to assess how results change. The problem with this approach is that when divergent results are found, third parties do not have clear grounds to decide which results to believe. A more coherent strategy facilitated by design declaration is to learn about what a study {\it could have} revealed under different analysis strategies, not just what the original author reports {\it was} revealed. Answering this questions provides a basis for supporting alternative analyses on the basis of the original authors' intentions and not on the basis of the degree of divergence of results. Conversely, it provides authors with grounds to question claims made by their critics. 

The third chapter focuses on research funders. We believe that design declaration can alter the way that research designs are evaluated. Currently funders of research have limited tools for assessing  research quality, relying largely on the motivation for the research and  the reputations of researches. Sometimes power analyses are requested though these are often difficult to interrogate. In contrast access to declared designs allows funders to directly interrogate a design, to check whether the design will provide reliable learning under different conditions or if operated at different scales, under assumptions provided by researchers. This chapter describes how such review and evaluation activities can be conducted with declared designs.



\section{Relation to existing works}


There are many excellent textbooks that deal with different features of design. Broadly these focus on different diemnsions of
The classic test \textit{Designing Social Inquiry} by King, Keohane, and Verba (KKV) considered research design as a general topic with a unifying framework for quantitative and qualitative reasoning. KKV, however, was published in 1994, and does not include many of the new connections across types of research designs generated by the causal identification literature.


John Gerring. Social Science Methodology: A Unified Framework. Cambridge University Press, New York, second edition, 2012.

Brady et al. Rethinking Social Inquiry: Diverse Tools, Shared Standards

Dimiter Toshkov. Research Design in Political Science. Palgrave Macmillan, 2016.

Gerber and Green

Glennerster RRE

Dunning natural experiments

An Introduction to Statistical Learning: with Applications in R (Springer Texts in Statistics)

Computer age statistical inference

\section{Audience}

\textit{DDRD} is not intended as a standalone textbook for a course, rather it serves as an accompanying text, which could be used in a wide variety of courses, and as a reference or researchers and research funders. 

Likely courses that DDRD could accompany include:

\begin{enumerate}
\item General research design courses in masters and Ph.D. courses in political science, public policy, sociology, and business schools in the U.S. and Europe

\item Experiments and causal inference quantitative methods courses in Ph.D. courses in political science, public policy, sociology

\item Advanced experiments and quantitative methods courses at top undergraduate institutions

\item Applied substantive courses in the social sciences that feature complex applied research designs
\end{enumerate}

For these classes the book could serve as a set of modules, focusing students on fundamentals of research design, or it could be used to accompany other statistical classes, providing tool for students to explore properties of any model that is introduced. Beyond methods courses, many graduate courses in the social sciences focus on published research with highly complex protocols and methods. The findings of such research are difficult to interpret without some understanding of the underlying design. Our short format design overviews could be given as handouts to provide an introduction to the underlying logic of the methods employed, as well as their advantages and disadvantages. 


Beyond the classroom the book is aimed at users---students and researchers---seeking to develop and understand research strategies and at evaluation practitioners -- impact evaluation and measurement and evaluation (M\&E) specialists in government, international lending institutions such as the World Bank, bilateral aid agencies, major donors such as the Hewlett Foundation, and political consulting organizations such as the Analyst Institute, who all support research but have few tools to assess the  integrity of research proposed to them.


\section{Timeline}

We expect to complete the book over the next 18 months, which corresponds to a grant that is funding research and programming assistance from the Laura and John Arnold Foundation.

By October 2018, we will complete a draft of the book material and hold a book conference (funded by the Arnold Foundation) shortly thereafter. We will then complete the remainder of the manuscript by April 2019.

\section{Enclosures}

We enclose:

\begin{enumerate}
\item Sample pages of containing two subsections of Part B (Simple random sampling, Regression Discontinuity Designs) and one subsection of Part C (Answering meaningful questions). 
\item A prototype of the e-book, including interactive applications that interrogate various research designs. This prototype can be viewed at \url{http://http://ddrd.declaredesign.org}; an example of the applications can be seen at LINK
\item A draft detailed table of contents
\item An unpublished article manuscript outlining the framework to be expanded upon in the book
\end{enumerate}	

\clearpage\newpage


\appendix
\section{Appendix: Detailed (Draft) Table of Contents \label{app:toc}}

\begin{legal}
\item {Part A: Declaring and Diagnosing Research Designs}

	\begin{legal}
	\item  MIDA, in principle, in practice
	\item  Diagnosing research designs
	\item  How to read this book
	\end{legal}

\item Part B: Design Library

	\begin{legal}
	\item Observational designs for descriptive inference: 
	
		\begin{legal}
		\item Simple Random Sampling
		\item Stratified clustered  random sampling
		\item Text analysis
		\item Latent variables
		\item RDS / Capture recapture
		\end{legal}
	
	\item Experimental designs for descriptive inference
	
		\begin{legal}
		\item 	Audit experiments
		\item 	List experiments
		\item 	Randomized responses
		\item 	Conjoint Experiments
		\item 	Two player lab experiments
		\end{legal}
	
	\item Experimental designs for causal inference
	
		\begin{legal}
		\item 	k-arm
		\item 	Block-Cluster randomized control trial
		\item 	Factorial design
		\item 	Encouragement design (IV)
		\item 	Rollout
		\item 	Partial population design for spillover analysis
		\item 	Selective trial
		\end{legal}

	\item Observational designs for causal inference
	
		\begin{legal}
		\item Difference in Differences
		\item 	Matching
		\item 	The Regression Discontinuity Design
		\item 	Synthetic control
		\item 	Process Tracing
		\item 	Cross national time series
		\end{legal}
	\end{legal}

\item Part C: Principles of Research Designs
	
	\begin{legal}
	\item Model Principles
	
		\begin{legal}
		\item 	Defining null models
		\item 	Make models general (Define models at the vector level)
		\item 	Spillovers are potential outcomes
		\item 	Compliance is a potential outcome
		\item 	Attrition is a potential outcome
		\item 	Specify your manipulands
		\end{legal}
		
	\item Inquiry Principles:
		
		\begin{legal}
					
		\item	No causation without manipulation
		\item	You are responsible for your estimand 
		\item	Know the A in ATE
		\item	Know the L in LATE 
		\item	PATEs are not SATEs
		\item	Inquiries can be open questions
		\item	Inquiries should have answers 
		\item	Answers to inquiries should be defined. 
		\item	Inquiries can be formed over inferences (though you probably don’t want to  do that)
		\item	The multiarm bandit: Decisions as inquiries. 
		\item	How many questions should I ask?
		\end{legal}
				
		\item Data Strategy Principles 

		\begin{legal}				
		\item	Systematic data collection is not random sampling
		\item	Maintain balance between treatment groups: Keep treatment and control groups parallel
		\item	Confounded treatments cause bias: (DD, RDD, factorial)
		\item	Power Principles: Blocking improves power
		\item	Power Principles: Clusters reduce power
		\item	Power Principles: Estimation strategies matter for power too
		\item	Power Principles:  Factorial designs are more powerful than m arm designs
		\item	Power Principles:  Gains from Pretreatment Data
		\item	Power Principles:  Should you take interim measures? 
		\end{legal}
		
    \item Analysis Strategy Principles 
		
    \begin{legal}
		\item (Balance fallacy) Assess imbalance using d not p
		\item 	Unbiasedness is not affected by imbalance but conditional bias is
		\item 	Use controls for prognostic pretreatment covariates
		\item 	Including pretreatment controls can introduce bias in observational analysis (Pearl)
		\item 	Conditioning on post treatment variables can introduce bias
		\item 	Heterogeneous propensities cannot be ignored:  OLS, IPW and HT
		\item 	A reasoned bases for inference 1:  Correctly identify the source of uncertainty. 
		\item 	Fisher's promise: Randomization inference is exact
		\item 	Permutation tests help with the sharp null of no effect; otherwise ri needs a model
		\item 	A reasoned bases for inference 3:  Neyman Variance is conservative for sample variance
		\item 	Cluster standard errors at the level of treatment assignment
		\item	A valid test need not be a powerful test 
		\item	Deaton's critique: Unbiased variance estimates do not mean correct p values
		\item	Confidence intervals should be model consistent
		\item	You can check whether confidence intervals are correct, given the model 
		\item	No evidence of an effect is not evidence of no effect
		\item	Gelman's lament. The difference between significant and not significant is not itself significant.
		\end{legal}
		
		\item Cross-MIDA Principles
		
		\begin{legal}
		\item	The substantive model should be more general than the answer strategy
		\item	As ye randomize so shall ye analyze
		\item	Don’t confuse observational and experimental variation
		\item	Selective reporting Introduces bias
		\item	There is a bias-variance tradeoffs
		\item	Treat spillovers as a design challenge
		\item	Validate your models
		\end{legal}
	\end{legal}
	
\item Part D: Putting Declared Designs to Use

	\begin{legal}
	\item  Researchers: Design development and registration
	\item  Peers: Better scholarly critique
	\item  Funders: Evaluating and supporting research 
	\end{legal}


\item Appendices
\end{legal}

\end{document}
