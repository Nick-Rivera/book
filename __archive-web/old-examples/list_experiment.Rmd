```{r,echo=FALSE,eval=FALSE}
tempR <- tempfile(fileext = ".R")
library(knitr)
purl("../index.Rmd", output=tempR)
source(tempR)
unlink(tempR)
```

Sometimes, subjects might not tell the truth when \textit{directly} asked about certain attitudes or behaviors. Responses may be affected by social desirability --- the desire to conform to a socially-expected response --- or fears of legal sanctions, for example. In such cases, standard survey estimates based on direct questions will be biased. One class of solutions to this problem is to obscure individual responses, providing protection from social or legal pressures. When we obscure responses systematically through an experiment, we can often still identify average quantities of interest. One such design is the list experiment, which obscures individual-level responses through aggregation by asking respondents for the count of the number of `yes' responses to a series of questions including the sensitive item. 

During the 2016 Presidential Election in the U.S., some observers were concerned that pre-election estimates of support for Donald Trump might have been downwardly biased by "Shy Trump Supporters" -- survey respondents who supported Trump in their hearts, but were embarrased to admit it to pollsters. To assess this possibility, Coppock (2017) obtained estimates of Trump support that were free of social desirability bias using a list experiment. Subjects in the control and treatment groups were asked: "Here is a list of [three/four] things that some people would do and some people would not. Please tell me HOW MANY of them you would do. We do not want to know which ones of these you would do, just how many. Here are the [three/four] things:"

| Control                                                                                           | Treatment                                                                                                                                                                |
| ------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour            | If it were up for a vote, I would vote to raise the minimum wage to 15 dollars an hour                                                                                   |
| If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare | If it were up for a vote, I would vote to repeal the Affordable Care Act, also known as Obamacare                                                                        |
| If it were up for a vote, I would vote to ban assault weapons                                     | If it were up for a vote, I would vote to ban assault weapons                                                                                                            |
|                                                                                                   | If the 2016 presidential election were being held today and the candidates were Hillary Clinton (Democrat) and Donald Trump (Republican), I would vote for Donald Trump. |

The treatment group averaged 1.843 items while the control group averaged 1.548 items, for a difference-in-means estimate of support for Donald Trump of 29.6\% (note that this estimate is representative of US adults and not of US adults who would actually vote). The trouble with this estimate is that, while it's plausibly free from social desirability bias, it's also much higher variance. The 95\% confidence interval for the list experiment estimate is nearly 14 percentage points wide, whereas the the 95\% confidence interval for the (possibly biased!) direct question asked of the same sample is closer to 4 percentage points.

The choice between list experiments and direct question is therefore a \textbf{bias-variance tradeoff}. List experiments may have less bias, but they are higher variance. Direct questions may be biased, but they have less variance.

### MIDA

- **M**odel: Our model includes subjects' true support for Donald Trump and whether or not they are "shy".  These two variables combine to determine how subjects will respond when asked directly about Trump support.

    The potential outcomes model combines three types of information to determine how subjects will respond to the list experiment: their responses to the three nonsensitive control items, their true support for Trump, and whether they are assigned to see the treatment or the control list. Notice that our definition of the potential outcomes embeds the "No Liars" and "No Design Effects" assumptions required for the list experiment design (see Blair and Imai 2012 for more on these assumptions).

    We also have a global parameter that reflects our expectations about the proportion of Trump supporters who are shy. It's set at 6%, which is large enough to make a difference for polling, but not so large as to be implausible.

- **I**nquiry: Our estimand is the proportion of voters who actually plan to vote for Trump. 

- **D**ata strategy: First we sample 500 respondents from the U.S. population at random, then we randomly assign 250 of the 500 to treatment and the remainder to control. In the survey, we ask subjects both the direct question and the list experiment question.

- **A**nswer strategy: We estimate the proportion of truthful Trump voters in two ways. First, we take the mean of answers to the direct question. Second, we take the difference in means in the responses to the list experiment question.

### Declaration

```{r}
# Model -------------------------------------------------------------------
proportion_shy <- .06

population <- declare_population(
  N = 5000,
  # true trump vote (unobservable)
  truthful_trump_vote = draw_binary(.45, N),
  
  # shy voter (unobservable)  
  shy = draw_binary(proportion_shy, N),
  
  # Direct question response (1 if Trump supporter and not shy, 0 otherwise)
  Y_direct = as.numeric(truthful_trump_vote == 1 & shy == 0),
  
  # Nonsensitive list experiment items
  raise_minimum_wage = draw_binary(.8, N),
  repeal_obamacare = draw_binary(.6, N),
  ban_assault_weapons = draw_binary(.5, N)
)

potential_outcomes <- declare_potential_outcomes(
  Y_list_Z_0 = raise_minimum_wage + repeal_obamacare + ban_assault_weapons,
  Y_list_Z_1 = Y_list_Z_0 + truthful_trump_vote
)

# Inquiry -----------------------------------------------------------------
estimand <- declare_estimand(
  proportion_truthful_trump_vote = mean(truthful_trump_vote))

# Data Strategy -----------------------------------------------------------
sampling <- declare_sampling(n = 500)
assignment <- declare_assignment(prob = .5)

# Answer Strategy ---------------------------------------------------------
estimator_direct <- declare_estimator(
  Y_direct ~ 1,
  model = lm_robust,
  coefficient_name = "(Intercept)",
  estimand = estimand,
  label = direct
  )

estimator_list <- declare_estimator(Y_list ~ Z,
                                    model = difference_in_means,
                                    estimand = estimand,
                                    label = list)

# Design ------------------------------------------------------------------
design <- declare_design(
  population, 
  potential_outcomes, 
  sampling,
  estimand,
  assignment, 
  reveal_outcomes(outcome_variable_names = Y_list),
  estimator_direct,
  estimator_list
  )
```

```{r, echo=FALSE, eval= rerun_templates}
diagnosis <- diagnose_design(design, sims = 1000, bootstrap = FALSE)
saveRDS(diagnosis, file = "../examples_data/05_list_experiments_intext.rdata")
```

### Takeaways

```{r plot, echo = FALSE, fig.width = 6, fig.height = 3}
#coppock_2017_plot <- get_estimates(design) %>% 
#  ggplot() +
#  geom_errorbarh(aes(x = est, y = estimator_label, xmin = ci_lower, xmax = ci_upper), height = 0) +
#  geom_point(aes(y = estimator_label, x = est)) +
#  dd_theme

load("../examples_data/05_list_experiments.rdata")

diagnosand_plot <- 
  diagnosis$diagnosands %>% 
  select(proportion_shy, estimator_label, bias, rmse) %>% 
  gather(key = diagnosand_label,
         value = diagnosand,
         bias,
         rmse) %>% 
  ggplot(aes(x = proportion_shy, y = diagnosand, 
             group = estimator_label, color = estimator_label)) +
  geom_line() +
  facet_wrap(~ diagnosand_label) +
  dd_theme


if (knitr::opts_knit$get("rmarkdown.pandoc.to") != "html") {
#  grid.arrange(coppock_2017_plot, diagnosand_plot, ncol = 2)
  diagnosand_plot
}
```

<iframe src="https://graemeblair.shinyapps.io/sensitive-questions/" style="border: none; width: 100%; height: 400px"></iframe>

We see that the list experiment has near-zero bias regardless of the proportion of shy respondents, but that it is less efficient than the direct question. 

### Mock data

```{r, echo = FALSE}
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "html") {
  dat <- draw_data(design)
  DT::datatable(dat, fillContainer = TRUE)
} 
```

### Exercises

1. The "No Liars" assumption states that subjects respond truthfully to the sensitive item on the treatment list. Change the design declaration to reflect a violation of "No Liars". How do violations of this assumption affect the tradeoff between list experiments and direct questions?

2. How large a sample size would you need in order for the list experiment to have a confidence interval that is four percentage points wide, on average?  (hint: the expected width of a confidence interval is a new diagnosand).
