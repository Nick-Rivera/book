[
["index.html", "Declaring and Diagnosing Research Designs What we are at", " Declaring and Diagnosing Research Designs Graeme Blair, Jasper Cooper, Alexander Coppock, Macartan Humphreys What we are at We have big hopes for this book. We hope to promote a new, comprehensive way of thinking about research designs in the social sciences. We hope this way of thinking will make research designs more transparent and more robust. But we also hope it will make research design easier, easier to produce good designs, but also easier to share designs and build off of the designs that others have developed. The core idea is to start think of a design as an object that can be interrogated. The design encodes your beliefs about the world, it describes your questions, and it lays out how you go about answering those questions, in terms both of what data you use and how you use it. A key idea is that all of these features can be provided in code and if done right the information provided is enough to be able to simulate a run of the design. being able to simulate a design puts a reearcher in a powerful position as you can then start assessing the conditions under which a design perfoms well or badly. "],
["observational-designs-for-descriptive-inference.html", "Part 1 Observational designs for descriptive inference ", " Part 1 Observational designs for descriptive inference "],
["simple-random-sampling.html", "1.1 Simple Random Sampling", " 1.1 Simple Random Sampling Often we are interested in features of a population, but data on the entire population is prohibitively expensive to collect. Instead, researchers obtain data on a small fraction of the population and use measurements taken on that sample to draw inferences about the population. Imagine we seek to estimate the average political ideology of residents of the small town of Portola, California, on a left-right scale that varies from 1 (most liberal) to 7 (most conservative). We draw a simple random sample in which all residents have an equal chance of inclusion in the study. It’s a straightforward design but formally declaring it will make it easy to assess its properties. 1.1.1 Design Declaration Model: Even for this most basic of designs, researchers bring to bear a background model of the world. As described in Chapter 1, the three elements of a model are the signature, probability distributions over variables, and functional equations among variables. The signature here is a specification of the variable of interest, \\(Y\\), with a well defined domain (seven possible values between 1 and 7). In the code declaration below, we assume a uniform distribution over these 7 values. This choice is a speculation about the population distribution of \\(Y\\); some features of the design diagnosis will depend on the choice of distribution. The functional equations seem absent here as there is only one variable in the model. We could consider an elaboration of the model that includes three variables: the true outcome, \\(Y\\); the decision to measure the outcome, \\(M\\); and the measured outcome, \\(Y^M\\). We ignore this complication for now under the assumption that \\(Y = Y^M\\), i.e., that \\(Y\\) is measured perfectly. Finally, the model also includes information about the size of the population. Portola, California, has a population of approximately 2100 people as of 2010, so \\(N = 2100\\). Inquiry: Our inquiry is the population mean of \\(Y\\): \\(\\frac{1}{N} \\sum_1^N Y_i = \\bar{Y}\\). Data strategy: In simple random sampling, we draw a random sample without replacement of size \\(n\\), where every member of the population has an equal probability of inclusion in the sample, \\(\\frac{n}{N}\\). When \\(N\\) is very large relative to \\(n\\), units are drawn approximately independently. In this design we measure \\(Y\\) for \\(n=100\\) units in the sample; the other \\(N-n\\) units are not measured. Answer strategy: We estimate the population mean with the sample mean estimator: \\(\\widehat{\\overline{Y}} = \\frac{1}{n} \\sum_1^n Y_i\\). Even though our inquiry implies our answer should be a single number, an answer strategy typically also provides statistics that help us assess the uncertainty around that single number. To construct a 95% confidence interval around our estimate, we calculate the standard error of the sample mean, then approximate the sampling distribution of the sample mean estimator using a formula that includes a finite population correction. In particular, we approximate the estimated sampling distribution by a \\(t\\) distribution with \\(n - 1\\) degrees of freedom. In the code for our answer strategy, we spell out each step in turn. # Model ------------------------------------------------------------------- N &lt;- 2100 fixed_population &lt;- declare_population(N = N, Y = sample(1:7, N, replace = TRUE))() population &lt;- declare_population(data = fixed_population) # Inquiry ----------------------------------------------------------------- estimand &lt;- declare_estimand(Ybar = mean(Y)) # Data Strategy ----------------------------------------------------------- n &lt;- 100 sampling &lt;- declare_sampling(n = n) # Answer Strategy --------------------------------------------------------- estimator &lt;- declare_estimator(Y ~ 1, model = lm_robust, estimand = estimand, label = &quot;Sample Mean Estimator&quot;) # Design ------------------------------------------------------------------ design &lt;- population + estimand + sampling + estimator diagnosands &lt;- declare_diagnosands(select = c(bias, coverage, mean_estimate, sd_estimate)) 1.1.2 Takeaways With the design declared we can run a diagnosis and plot results from Monte Carlo simulations of the design: diagnosis &lt;- diagnose_design( design, sims = 10000, bootstrap_sims = 1000, diagnosands = diagnosands) The diagnosis indicates that under simple random sampling, the sample mean estimator of the population mean is unbiased. The graph on the left shows the sampling distribution of the estimator: it’s centered directly on the true value of the inquiry. Confidence intervals also have a sampling distribution – they change depending on the idiosyncrasies of each sample we happen to draw. The figure on the right shows that the 95% of the time the confidence intervals cover the true value of the estimand, as they should. As sample size grows, the sampling distribution of the estimator gets tighter, but the coverage of the confidence intervals stays at 95% – just the properties we would want out of our answer strategy. Things work well here it seems. In the exercises we suggest some small modifications of the design that point to conditions under which things might break down. 1.1.3 Exercises Modify the declaration to change the distribution of \\(Y\\) from being uniform to something else: perhaps imagine that more extreme ideologies are more prevalent than moderate ones. Is the sample mean estimator still unbiased? Interpret your answer. Change the sampling procedure to favor units with higher values of ideology. Is the sample mean estimator still unbiased? Interpret your answer. Modify the estimation function to use this formula for the standard error: \\(\\widehat{se} \\equiv \\frac{\\widehat\\sigma}{\\sqrt{n}}\\). This equation differs from the one used in our declaration (it ignores the total population size \\(N\\)). Check that the coverage of this new design is incorrect when \\(N=n\\). Assess how large \\(N\\) has to be for the difference between these procedures not to matter. 1.1.4 Design Inspector "],
["observational-designs-for-causal-inference.html", "Part 2 Observational designs for causal inference ", " Part 2 Observational designs for causal inference "],
["regression-discontinuity.html", "2.1 Regression Discontinuity", " 2.1 Regression Discontinuity Regression discontinuity designs exploit substantive knowledge that treatment is assigned in a particular way: everyone above a threshold is assigned to treatment and everyone below it is not. Even though researchers do not control the assignment, substantive knowledge about the threshold serves as a basis for a strong identification claim. Thistlewhite and Campbell introduced the regression discontinuity design in the 1960s to study the impact of scholarships on academic success. Their insight was that students with a test score just above a scholarship cutoff were plausibly comparable to students whose scores were just below the cutoff, so any differences in future academic success could be attributed to the scholarship itself. Regression discontinuity designs identify a local average treatment effect: the average effect of treatment exactly at the cutoff. The main trouble with the design is that there is vanishingly little data exactly at the cutoff, so any answer strategy needs to use data that is some distance away from the cutoff. The further away from the cutoff we move, the larger the threat of bias. We’ll consider an application of the regression discontinuity design that examines party incumbency advantage – the effect of a party winning an election on its vote margin in the next election. 2.1.1 Design Declaration Model: Regression discontinuity designs have four components: A running variable, a cutoff, a treatment variable, and an outcome. The cutoff determines which units are treated depending on the value of the running variable. In our example, the running variable \\(X\\) is the Democratic party’s margin of victory at time \\(t-1\\); and the treatment, \\(Z\\), is whether the Democratic party won the election in time \\(t-1\\). The outcome, \\(Y\\), is the Democratic vote margin at time \\(t\\). We’ll consider a population of 1,000 of these pairs of elections. A major assumption required for regression discontinuity is that the conditional expectation functions for both treatment and control potential outcomes are continuous at the cutoff.1 To satisfy this assumption, we specify two smooth conditional expectation functions, one for each potential outcome. The figure plots \\(Y\\) (the Democratic vote margin at time \\(t\\)) against \\(X\\) (the margin at time \\(t-1\\)). We’ve also plotted the true conditional expectation functions for the treated and control potential outcomes. The solid lines correspond to the observed data and the dashed lines correspond to the unobserved data. Inquiry: Our estimand is the effect of a Democratic win in an election on the Democratic vote margin of the next election, when the Democratic vote margin of the first election is zero. Formally, it is the difference in the conditional expectation functions of the control and treatment potential outcomes when the running variable is exactly zero. The black vertical line in the plot shows this difference. Data strategy: We collect data on the Democratic vote share at time \\(t-1\\) and time \\(t\\) for all 1,000 pairs of elections. There is no sampling or random assignment. Answer strategy: We will approximate the treated and untreated conditional expectation functions to the left and right of the cutoff using a flexible regression specification estimated via OLS. In particular, we fit each regression using a fourth-order polynomial. Much of the literature on regression discontinuity designs focuses on the tradeoffs among answer strategies, with many analysts recommending against higher-order polynomial regression specifications. We use one here to highlight how well such an answer strategy does when it matches the functional form in the model. We discuss alternative estimators in the exercises. # Model ------------------------------------------------------------------- cutoff &lt;- .5 control &lt;- function(X) { as.vector(poly(X, 4, raw = T) %*% c(.7, -.8, .5, 1))} treatment &lt;- function(X) { as.vector(poly(X, 4, raw = T) %*% c(0, -1.5, .5, .8)) + .15} population &lt;- declare_population( N = 1000, X = runif(N,0,1) - cutoff, noise = rnorm(N,0,.1), Z = 1 * (X &gt; 0) ) potential_outcomes &lt;- declare_potential_outcomes( Y_Z_0 = control(X) + noise, Y_Z_1 = treatment(X) + noise) # Inquiry ----------------------------------------------------------------- estimand &lt;- declare_estimand(LATE = treatment(0) - control(0)) # Answer Strategy --------------------------------------------------------- estimator &lt;- declare_estimator( formula = Y ~ poly(X, 4) * Z, model = lm, estimand = estimand) # Design ------------------------------------------------------------------ design &lt;- population + potential_outcomes + estimand + reveal_outcomes + estimator 2.1.2 Takeaways diagnosis &lt;- diagnose_design(rd_design, sims = 10000, bootstrap_sims = 1000) We highlight three takeaways. First, the power of this design is very low: with 1,000 units we do not achieve even 10% statistical power. However, our estimates of the uncertainty are not too wide: the coverage probability indicates that our confidence intervals indeed contain the estimand 95% of the time as they should. Our answer strategy is highly uncertain because the fourth-order polynomial specification in regression model gives weights to the data that greatly increase the variance of the estimator (Gelman and Imbens (2017)). In the exercises we explore alternative answer strategies that perform better. Second, the design is biased because polynomial approximations of the average effect at exactly the point of the threshold will be inaccurate in small samples (Sekhon and Titiunik (2017)), especially as units farther away from the cutoff are incorporated into the answer strategy. We know that the estimated bias is not due to simulation error by examining the bootstrapped standard error of the bias estimates. Finally, from the figure, we can see how poorly the average effect at the threshold approximates the average effect for all units. The average treatment effect among the treated (to the right of the threshold in the figure) is negative, whereas at the threshold it is positive. This clarifies that the estimand of the regression discontinuity design, the difference at the cutoff, is only relevant for a small – and possibly empty – set of units very close to the cutoff. 2.1.3 Further Reading Since its rediscovery by social scientists in the late 1990s, the regression discontinuity design has been widely used to study diverse causal effects such as: prison on recidivism (Mitchell et al. (2017)); China’s one child policy on human capital (Qin, Zhuang, and Yang (2017)); eligibility for World Bank loans on political liberalization (Carnegie and Samii (2017)); and anti-discrimination laws on minority employment (Hahn, Todd, and Van der Klaauw (1999)). We’ve discussed a “sharp” regression discontinuity design in which all units above the threshold were treated and all units below were untreated. In fuzzy regression discontinuity designs, some units above the cutoff remain untreated or some units below take treatment. This setting is analogous to experiments that experience noncompliance and may require instrumental variables approaches to the answer strategy (see Compliance is a Potential Outcome). Geographic regression discontinuity designs use distance to a border as the running variable: units on one side of the border are treated and units on the other are untreated. Keele and Titiunik (2016) use such a design to study whether voters are more likely to turn out when they have the opportunity to vote directly on legislation on so-called ballot initiatives. A complication of this design is how to measure distance to the border in two dimensions. 2.1.4 Exercises Gelman and Imbens (2017) point out that higher order polynomial regression specifications lead to extreme regression weights. One approach to obtaining better estimates is to select a bandwidth, \\(h\\), around the cutoff, and run a linear regression. Declare a sampling procedure that subsets the data to a bandwidth around the threshold, as well as a first order linear regression specification, and analyze how the power, bias, RMSE, and coverage of the design vary as a function of the bandwidth. The rdrobust estimator in the rdrobust package implements a local polynomial estimator that automatically selects a bandwidth for the RD analysis and bias-corrected confidence intervals. Declare another estimator using the rdrobust function and add it to the design. How does the coverage and bias of this estimator compare to the regression approaches declared above? Reduce the number of polynomial terms of the the treatment() and control() functions and assess how the bias of the design changes as the potential outcomes become increasingly linear as a function of the running variable. Redefine the population function so that units with higher potential outcome are more likely to locate just above the cutoff than below it. Assess whether and how this affects the bias of the design. 2.1.5 Design Inspector References "],
["inquiry-principles.html", "Part 3 Inquiry principles ", " Part 3 Inquiry principles "],
["some-designs-have-badly-posed-questions-but-design-diagnosis-can-alert-you-to-the-problem.html", "3.1 Some designs have badly posed questions but design diagnosis can alert you to the problem", " 3.1 Some designs have badly posed questions but design diagnosis can alert you to the problem An obvious requirement of a good research design is that the question it seeks to answer does in fact have an answer, at least under plausible models of the world. Interestingly, we can sometimes get quite far along a research path without being conscious that the questions we ask do not have answers and the answers we get are answering different questions. Fortunately, computers complain when they are asked to answer badly posted questions. How could a question not have an answer? Answerless questions can arise when inquiries depend on variables that do not exist or are undefined for some units. Consider an audit experiment that seeks to assess the effects of an email from a Latino name (versus a White name) on whether and how well election officials respond to requests for information. For example, do they use a positive or negative tone. These questions seem reasonable enough. The problem, however, is that if there are officials who don’t send responses, tone is undefined. More subtly, if there is an official that does send an email but would not have sent it in a different treatment condition, then tone is undefined for one of their potential outcomes. 3.1.1 A design with sometimes undefined outcomes Here are the key parts to the design: Model: The model has two outcome variables, \\(R_i\\) and \\(Y_i\\). \\(R_i\\) stands for “response” and is equal to 1 if a response is sent, and 0 otherwise. \\(Y_i\\) is the tone of the response and is normally distributed when it is defined. \\(Z_i\\) is the treatment and equals 1 if the email is sent using a Latino name and 0 otherwise. The table below shows the potential outcomes for four possible types of subjects, depending on the potential outcomes of \\(R_i\\). A types respond if and only if they are not treated B types respond if and only if they are treated C types never respond, regardless of treatment D types always respond regardless of treatment The table also includes columns for the potential outcomes of \\(Y_i\\), showing which potential outcome subjects would express depending on their type. The key thing to note is that for the A, B, and C types, the effect of treatment on \\(Y_i\\) is undefined because messages never sent have no tone. The last (and very important) feature of our model is that the outcomes \\(Y_i\\) are possibly correlated with subject type. Even though both \\(E[Y_i(1) | \\text{Type} = D]\\) and \\(E[Y_i(1) | \\text{Type} = B]\\) exist, there’s no reason to expect that they are the same. Causal Types Type \\(R_i(0)\\) \\(R_i(1)\\) \\(E(Y_i(0))\\) \\(E(Y_i(1))\\) A 1 0 -1 NA B 0 1 NA -1 C 0 0 NA NA D 1 1 0 1 Inquiry: We have two inquiries. The first is straightforward: \\(E[R_i(1) - R_i(0)]\\) is the Average Treatment Effect on response. The second inquiry is the undefined inquiry that does not have an answer: \\(E[Y_i(1) - Y_i(0)]\\). We will also consider a third inquiry, which is defined: \\(E[Y_i(1) - Y_i(0) | \\mathrm{Type} = D]\\), which is the average effect of treatment on tone among \\(D\\) types. Data strategy: The data strategy will be to use complete random assignment to assign 250 of 500 units to treatment. Answer strategy: We’ll try to answer all three inquiries with the difference-in-means estimator. This design can be declared formally like this: # Model ------------------------------------------------------------------- population &lt;- declare_population( N = 500, noise = rnorm(N), type = sample(1:4, N, replace = TRUE, prob = c(0, 1/3, 1/3, 1/3)), A = type == 1, B = type == 2, C = type == 3, D = type == 4) pos &lt;- declare_potential_outcomes( R_Z_0 = A|D, # A and D types report in the control condition R_Z_1 = B|D, # B and D types report in the treatment condition Y_Z_0 = ifelse(R_Z_0, 0*D - 1*A + noise, NA), Y_Z_1 = ifelse(R_Z_1, 1*D - 1*B + noise, NA)) # Inquiry ----------------------------------------------------------------- estimands &lt;- declare_estimand( ATE_R = mean(R_Z_1 - R_Z_0), ATE_Y = mean(Y_Z_1 - Y_Z_0), ATE_Y_for_Ds = mean(Y_Z_1[D] - Y_Z_0[D])) # Data Strategy ----------------------------------------------------------- assignment &lt;- declare_assignment() reveal &lt;- declare_reveal(outcome_variables = c(&quot;R&quot;, &quot;Y&quot;)) # Answer Strategy --------------------------------------------------------- est1 &lt;- declare_estimator(R ~ Z, estimand = &quot;ATE_R&quot;, label = &quot;DIM_R&quot;) est2 &lt;- declare_estimator(Y ~ Z, estimand = list(&quot;ATE_Y&quot;, &quot;ATE_Y_for_Ds&quot;), label = &quot;DIM_Y&quot;) # Design ------------------------------------------------------------------ design &lt;- population + pos + estimands + assignment + reveal + est1 + est2 3.1.2 What we find from diagnosis Here is a diagnosis of this design: diagnosis &lt;- diagnose_design(design, sims = sims) Design Label Estimator Label Term N Sims Bias RMSE Power Coverage Mean Estimate SD Estimate Mean Se Type S Rate Mean Estimand design DIM_R Z 50 0.00 0.04 1.00 1.00 0.34 0.04 0.04 0.00 0.33 (0.00) (0.00) (0.00) (0.00) (0.01) (0.00) (0.00) (0.00) (0.00) design DIM_Y Z 50 NA NA 0.08 NA -0.01 0.16 0.15 NA NA NA NA (0.04) NA (0.02) (0.02) (0.00) NA NA design DIM_Y Z 50 -1.01 1.02 0.08 0.00 -0.01 0.16 0.15 0.75 1.00 (0.02) (0.02) (0.04) (0.00) (0.02) (0.02) (0.00) NA (0.00) We learn three things from the design diagnosis. First, as expected, our experiment is unbiased for the average treatment effect on response. Second, our second inquiry, as well as our diagnostics for it, are undefined. The diagnosis tells us that our definition of potential outcomes produces a definition problem for the estimand. Note that some diagnosands are defined, including power. These are diagnosands that depend only on the answer strategy and not on the estimand. The third estimand – the average effects for the \\(D\\) types – is defined but our estimates are biased. The reason for this is that we cannot tell from the data which types are the \\(D\\) types: we are not conditioning on the correct subset. Indeed, we are unable to condition on the correct subset. If a subject responds in the treatment group, we don’t know if she is an \\(B\\) or a \\(D\\) type; in the control group, we can’t tell if a responder is an \\(A\\) or a \\(D\\) type. Our difference-in-means estimator of the ATE on \\(Y\\) among \\(D\\)s will be off whenever \\(D\\)s have different outcomes from \\(A\\)s and \\(B\\)s. There are some solutions to this problem. In some cases, the problem might be resolved by changing the inquiry. Closely related estimands can often be defined, perhaps by redefining \\(Y\\) (e.g., emails never sent have a tone of zero). Some redefinitions of the problem, as in the one we examine above, require estimating effects for unobserved subgroups which is a difficult challenge. 3.1.3 Other instances of this problem This kind of problem is surprisingly common. Here are three more instances of the problem: \\(Y\\) is the decision to vote Democrat (\\(Y=1\\)) or Republican (\\(Y=0\\)), \\(R\\) is the decision to turn out to vote and \\(Z\\) is a campaign message. The decision to vote may depend on treatment but if subjects do not vote then \\(Y\\) is undefined. \\(Y\\) is the weight of infants, \\(R\\) is whether a child is born and \\(Z\\) is a maternal health intervention. Fertility may depend on treatment but the weight of unborn (possibly never conceived) babies is not defined. \\(Y\\) is the charity to whom contributions are made during fundraising and \\(R\\) is whether anything is contributed and \\(Z\\) is an encouragement to contribute. The identity of beneficiaries is not defined if there are no contributions. All of these problem exhibit a form of post treatment bias (see section Post treatment bias) but the issue goes beyond picking the right estimator. Our problem here is conceptual: the effect of treatment on the outcome just doesn’t exist for some subjects. 3.1.4 Keep thinking Some puzzles: The amount of bias on the third estimand depends on both the distribution of types and the correlation of types with the potential outcomes of Y. Modify the declaration so that the estimator of the effect on Y is unbiased, changing only the distribution of types. Repeat the exercise, changing only the relation between the types and the potential outcomes of \\(Y\\). Try approaching the problem by redefining the inquiry, seeking to assess the effect of treatment on the share of responses with positive tone. "],
["references.html", "References", " References "]
]
