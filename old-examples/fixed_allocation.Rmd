```{r,echo=FALSE,eval=FALSE}
tempR <- tempfile(fileext = ".R")
library(knitr)
purl("../index.Rmd", output=tempR)
source(tempR)
unlink(tempR)
```

Sometimes researchers take a fixed number of subjects from groups of varying sizes and allocate them to treatment. The technical term for the groups in this context is **blocks**, i.e., any group in which the treatment is assigned to some units and not to others, independently of the assignment of units to treatment within other groups in the experiment. This deceptively simple design can actually lead to damaging bias if not analyzed carefully. 

In the lead up to the Illinois 5th Congressional District Special Election in 2009, researchers Betsy Sinclair, Margaret McConnell and Donald P. Green assigned one person in one-, two- and three-person households to receive a letter reminding them whether they had voted in the past and imploring them to do their civic duty and vote in the upcoming election. If  you were a subject in their study and $m$ of the $N$ households in your area were randomly assigned to treatment, your chances of being assigned to treatment would be $m/N$ if you lived alone, $(m/N)/2$ if you lived with two other people, and $(m/N)/3$ if you lived with two people. In studies like this, units in smaller groups are more likely to be assigned to treatment. If the size of the group is somehow related to the outcome we are studying, our estimates of the effect of the treatment will be biased unless we account for the negative correlation between group size and probability of assignment to treatment. Accounting for this correlation reduces bias but increases our uncertainty, leading to consequential tradeoffs between bias and variance. 

### MIDA 

- **M**odel: To illustrate how correlation between group size and outcomes matters in we declare three different models of the world. In the first, the treatment effect is simply 1 for all units, and the size of the group has no effect on the outcome (potential outcomes: `no_size_effect`). In the second, the size of the group is correlated with the outcome, but does not alter the size of the treatment effect (potential outcomes: `size_schock`). In the third, the size of the effect is equal to the group size. 
- **I**nquiry: We want to know the true average effect of the treatment in our sample. 
- **D**ata strategy: We randomly assign one unit in each group to treatment.
- **A**nswer strategy: We compare the performane of three different answer strategies in each design. The first simply regresses the outcome on the treatment indicator to obtain the unweighted difference in means (DIM). The second estimates a different intercept for each group using 'block fixed effects' (BFE). The third also uses block fixed effects, but also inversely weights the observations by the probability that they are observed in the condition to which they were assigned (IPW-BFE). [A bit more here about what ipw are?]

### Declaration

```{r,eval = FALSE,size="scriptsize"}
# Model -------------------------------------------------------------------
population <- 
  declare_population(group = level(N = 50, 
                                   size = sample(1:3, N, TRUE)),
                     individual = level(N = size,
                                        noise = rnorm(N)))
no_size_correlation <- declare_potential_outcomes(formula = Y ~ Z + noise * 2)
size_intercept <- declare_potential_outcomes(formula = Y ~ size + Z + noise)
size_effect <- declare_potential_outcomes(formula = Y ~ size * Z + noise)
# Inquiry -----------------------------------------------------------------
estimand <- declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0))
# Data Strategy -----------------------------------------------------------
assignment <- declare_assignment(block_var = group, m = 1)
# Answer Strategy ---------------------------------------------------------
dim <- declare_estimator(formula = Y ~ Z,
                         model = lm,
                         coefficient_name = "Z",
                         label = "DIM", 
                         estimand = estimand)
bfe <- declare_estimator(formula = Y ~ Z + group,
                         model = lm,
                         coefficient_name = "Z",
                         label = "BFE", 
                         estimand = estimand)
ipw_bfe <- declare_estimator(formula = Y ~ Z + group,
                             model = lm,
                             coefficient_name = "Z",
                             label = "IPW-BFE",
                             weights = 1 / Z_cond_prob, 
                             estimand = estimand)
# Design declaration ------------------------------------------------------
no_size_correlation_design <- declare_design(
  population, no_size_effect, estimand, assignment, reveal_outcomes,
  dim, bfe, ipw_bfe
)
size_intercept_design <- declare_design(
  population, size_schock, estimand, assignment, reveal_outcomes,
  dim, bfe, ipw_bfe
)
size_effect_design <- declare_design(
  population, size_effect, estimand, assignment, reveal_outcomes,
  dim, bfe, ipw_bfe
)
```

```{r, eval=FALSE}
diagnosis <- 
  diagnose_design(no_size_correlation_design = no_size_correlation_design, 
                  size_intercept_design = size_intercept_design,
                  size_effect_design = size_effect_design,
                  sims = sims, bootstrap = FALSE)
```

### Takeaways  

```{r fixed_allocation, echo=FALSE, fig.cap="Errors under different fixed allocation designs", fig.width=7, fig.height=3,}

dummy_frame <- fabricate(
  design = level(N = 3, design_label = factor(x = 1:3,levels = 1:3,labels = c("M: No size correlation", "M: Size & intercept correlated","M: Size & effect correlated"))),
  estimator = level(N = 3, estimator_id = 1:3, estimator_label = c("DIM", "BFE","IPW-BFE")),
  sim = level(N = 1000, noise = rnorm(N), error = noise + (design == 2&estimator_id == 1)*rnorm(N,1,1) +  (design == 3&estimator_id %in% 1:2)*rnorm(N,1,1) + rnorm(N,0,2)*(estimator_id == 3) )
)

ggplot(dummy_frame,aes(color = estimator_label, linetype = estimator_label,group = estimator_label,  x = error)) + 
  geom_density() + 
  theme_bw() +
  scale_color_discrete(name = "") +
  scale_linetype_discrete(name = "") +
  coord_cartesian(xlim = c(-8,8)) +
  geom_vline(aes(xintercept = mean_error, 
                 color = estimator_label
                 ),
             size = .2,
             data = plyr::ddply(.data = dummy_frame,
                          .variables = plyr::.(estimator_label,design_label),
                          mean_error = mean(error),
                          .fun = summarize)) +
  facet_wrap(facets =  ~ design_label) +
  theme(strip.background = element_blank())
  

```

The figure illustrates how the correlation in group size and assignment probabilities can affect our inferences. Three points are important to note: 

- The errors of the inverse-probability weighted estimator are always centered at 0 but widely dispersed: this means that it is unbiased, but produces more uncertainty than the other approaches. 

- Unsurprisingly, the difference-in-means estimator is only unbiased when there is no correlation between group size and outcomes. Here, the bias in the second and third designs is negative because being in a big group means that your treated potential outcome is higher and that you're less likely to be in treatment. 

- When block sizes are correlated with outcomes, but treatment effects are not, as in the second design, the block fixed effects estimator outperforms the inverse-probability weighted estimator because it is both unbiased and less noisy. However, the block fixed effects only account for differences in intercept: when treatment effects are bigger or smaller depending on block size, then the block fixed effects estimator is biased (although less so than the DIM estimator). The conservative approach in this case is to do X.

### Exercises










































